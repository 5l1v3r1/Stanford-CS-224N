{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 WRITTEN: Understanding word2vec (23 points)##\n",
    "\n",
    "Let’s have a quick refresher on the *word2vec* algorithm.  The key insight behind *word2vec* is that *a word is known by the company it keeps*.  Concretely, suppose we have a ‘center’ word $c$ and a contextual window surrounding $c$.  We shall refer to words that lie in this contextual window as ‘outside words’.  For example, in **Figure 1** we see that the center word $c$ is ‘banking’.  Since the context window size is 2, the outside words are ‘turning’, ‘into’, ‘crises’, and ‘as’.The goal of the skip-gram *word2vec* algorithm is to accurately learn the probability distribution $P(O | C)$. Given a specific word $o$ and a specific word $c$, we want to calculate $P(O = o|C = c)$, which is the probability that word $o$ is an ‘outside’ word for $c$, i.e., the probability that $o$ falls within the contextual window of $c$.\n",
    "\n",
    "> (Image to be added)\n",
    "> **Figure 1**\n",
    "\n",
    "In *word2vec*, the conditional probability distribution is given by taking vector dot-products and applying the softmax function:\n",
    "\n",
    "\\begin{aligned} P(O = o | C = c) = \\frac{exp(u^{T}_{o}v_{c})}{\\sum_{w\\in  Vocab}exp(u^{T}_{w}v_{c})} \\end{aligned}\n",
    "\n",
    "\n",
    "Recall from lectures that, for a single pair of words $c$ and $o$, the loss is given by:\n",
    "\n",
    "\n",
    "\\begin{aligned} J_{naive-softmax}(v_{c}, o, U) = - log P(O = o | C = c) \\end{aligned}\n",
    "\n",
    "\n",
    "Another way to view this loss is as the cross-entropy (2) between the true distribution $y$ and the predicted distribution $\\hat{y}$.  Here, both $y$ and $\\hat{y}$ are vectors with length equal to the number of words in the vocabulary. Furthermore, the $k$th entry in these vectors indicates the conditional probability of the $k$th word being an‘outside word’ for the given $c$.  The true empirical distribution $y$ is a one-hot vector with a 1 for the true out-side word $o$, and 0 everywhere else.  The predicted distribution $\\hat{y}$ is the probability distribution $P(O | C = c)$ given by our model in the first equation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>\n",
    "(1) Assume that every word in our vocabulary is matched to an integer numberk. Bolded lowercase letters represent vectors $_{.uk}$ is both the $kth$ column ofUand the ‘outside’ word vector for the word indexed by $k_{.vkis}$ both the $kth$ column of $V$ and the ‘center’ word vector for the word indexed by $k$.In order to simplify notation we shall interchangeably use $k$ to refer to the word and the index-of-the-word.\n",
    "\n",
    "\n",
    "(2) The Cross Enthropy Loss Between the true (discrete) probability distribution $p$ and another distribution $q$ is $ - \\sum_{i} p_{i} \\log(q_{i})$.\n",
    "</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)  (3 points)\n",
    "\n",
    "Show that the native-softmax loss given in the second equation above is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e. show that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (5 points)\n",
    "\n",
    "Compute the partial deriviative of $J_{native-softmax}(v_{c}, o, U)$ with reqpect to $v_{c}$.  Please write your answer in terms of $y$, $\\hat{y}$, and $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) (5 points)\n",
    "\n",
    "Compute the partial derivatives of $J_{naive-softmax}(v_{c},o,U)$ with respect to each of the ‘outside’ word vectors, $u_{w}$’s.  There will be two cases:  when $w = o$, the true ‘outside’ word vector, and $w != o$, forall other words.  Please write you answer in terms of $y$, $\\hat{y}$, and $v_{c}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) (3 points)\n",
    "\n",
    "The sigmoid function is given by equation 4:\n",
    "\n",
    "\\begin{aligned} \\frac{\\partial}{\\partial x}\\sigma(x) = \\frac{1}{1 + e^{-1x}} \\frac{e^x}{e^x + 1} \\end{aligned}\n",
    "\n",
    "Please compute the derivative of $σ(x)$ with respect to $x$, where $x$ is a scalar.  \n",
    "Hint:  you may want to write your answer in terms of $σ(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) (4 points)\n",
    "\n",
    "Now  we  shall  consider  the  Negative  Sampling  loss,  which  is  an  alternative  to  the  Naive Softmax loss.  Assume that $K$ negative samples (words) are drawn from the vocabulary.  For simplicity of notation we shall refer to them as $w_{1},w_{2},...,w_{K}$ and their outside vectors as $u_{1},...,u_{K}$.  Note that $o /∈ [w_{1},...,w_{K}]$.  For a center wordcand an outside word $o$, the negative sampling loss function is given by:\n",
    "\n",
    "\\begin{aligned} J_{neg-sample}(v_c, o, U) = - \\log(\\sigma(u^{T}_{o}v_{c})) - \\sum_{k = 1}^K \\log(\\sigma(-u^{T}_k v_c))\\end{aligned}\n",
    "\n",
    "for a sample $w_{1},...w_{K}$, where $σ(·)$ is the sigmoid function. (3)\n",
    "\n",
    "Please repeat parts (b) and (c), computing the partial derivatives of $J_{neg-sample}$ with respect to $v_{c}$, with respect  to $u_{o}$,  and  with  respect  to  a  negative  sample $u_{k}$.   Please  write  your  answers  in  terms  of  the vectors $u_{o}, v_{c}$, and $u_{k}$, where $k ∈ [1, K]$.  After you’ve done this, describe with one sentence why this loss function is much more efficient to compute than the naive-softmax loss.  Note, you should be able to use your solution to part (d) to help compute the necessary gradients here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) (3 pointts)\n",
    "\n",
    "Suppose the center word is $c = w_{t}$ and the context indow is $[w_{t-m}, ..., w_{t - 1}, w_{t_{n}}, w_{t +1}, ..., w_{t+ m}]$, where $m$ is the context window size.  Recall that for the skip-gram version of _word2vec_, the total loss for the context window is:\n",
    "\n",
    "\\begin{aligned} J_{skip-gram} (v_c, w_{t - m}, ... , w_{tt + m}, U) = \\sum_{-m \\leq j \\leq m \\\\ j \\neq 0}J(v_c, w_{t + j}, U) \\end{aligned}\n",
    "\n",
    "Here, $J(v_{c},w_{t+j}, U)$  represents  an  arbitrary  loss  term  for  the  center  word $c = w_{t}$ and  outside  word $w_{t+j}$.  $J(v_{c},w_{t+j}, U)$ could be $J_{naive-softmax}(v_{c},w_{t+j}, U)$ or $J_{neg-sample}(v_{c},w_{t+j}, U)$, depending on your implementation.  Write down three partial derivatives:\n",
    "\n",
    "\\begin{aligned}\\frac{∂ J_{skip-gram}(v_{c},w_{t−m},...w_{t+m}, U)}{∂U} \\\\\n",
    "\\frac{∂ J_{skip-gram}(v_{c},w_{t−m},...w_{t+m}, U)}{∂v_{c}} \\\\\n",
    "\\frac{∂ J_{skip-gram}(v_{c},w_{t−m},...w_{t+m},U)}{∂v_{w}} \\\\ w != c \\end{aligned} \n",
    "    \n",
    "Write  your  answers  in  terms  of $\\frac{∂ J(v_{c},w_{t+j}, U)}{∂U}$ and $\\frac{∂ J(v_{c},w_{t+j}, U)}{∂v_{c}}$.   This  is  very  simple  –each solution should be one line.  Once you’re done:\n",
    "    \n",
    "Given  that  you  computed the  derivatives  of $J(v_{c},w_{t+j}, U)$ with  respect to  all the model  parameters $U$ and $V$ in  parts  (a)  to  (c),  you  have  now  computed  the  derivatives  of  the  full  loss function $J_{skip-gram}$ with respect to all parameters.  You’re ready to implement *word2vec*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) CODING: Implementing word2vec (20 points)##\n",
    "\n",
    "In this part you will implement the *word2vec* model and train your own word vectors with stochastic gradient descent (SGD).  Before you begin, first run the following commmands within the assignment directory in order to create the appropriate conda virtual environment. (In this case, Jupyter Notebooks is being used).  This guarantees that you have all the necessary packages to complete the assignment.  Also note that you probably want to finish the previous math section before writing the code since you will be asked to implement the math functions in Python.  You want to implement and test the following subsections in order since they are accumulative.\n",
    "\n",
    "```conda env create -f env.yml```\n",
    "```conda activate a2```\n",
    "\n",
    "Once you are done with the assignment you can deactivate the environment by running:\n",
    "\n",
    "```conda deactivate```\n",
    "\n",
    "For each of the methods you need to implment, we include approximately how many lines of code our solution has in the code comments.  These numbers are included to guide you.  You don't have to stick to them, you can write shorter or longer code as you wish.  If you think your implementation is significantly longer than ours, it is a signal that there are some numpy methods you could utilize to make your code both shorter and faster.  for loops in python take a long time to complete when used over large arrays, so we expect you to utilize numpy methods.  We will be checking the efficiency of your code.  You will be able to see the results of the autograder when you submit your code to Gradescore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (12 points)\n",
    "\n",
    "We will start by implementing methods in *word2vec.py*.  First, implement the *sigmoid* method, which takes in a vector and applies the sigmoid function to it.  Then implement the softmax loss and gradient in the *naiveSoftmaxLossandGradient* method.  Finally, fill in the implementation for the skip-gram model in the s*skipgram* method.  When you are done, test your implementation by running python *word2vec.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from utils.gradcheck import gradcheck_naive, grad_tests_softmax, grad_tests_negsamp\n",
    "from utils.utils import normalizeRows, softmax\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE (~1 Line)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def naiveSoftmaxLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset\n",
    "):\n",
    "    \"\"\" Naive Softmax loss & gradient function for word2vec models\n",
    "\n",
    "    Implement the naive softmax loss and gradients between a center word's \n",
    "    embedding and an outside word's embedding. This will be the building block\n",
    "    for our word2vec models.\n",
    "\n",
    "    Arguments:\n",
    "    centerWordVec -- numpy ndarray, center word's embedding\n",
    "                    in shape (word vector length, )\n",
    "                    (v_c in the pdf handout)\n",
    "    outsideWordIdx -- integer, the index of the outside word\n",
    "                    (o of u_o in the pdf handout)\n",
    "    outsideVectors -- outside vectors is\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    for all words in vocab (U in the pdf handout)\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    loss -- naive softmax loss\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (word vector length, )\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE (~6-8 Lines)\n",
    "\n",
    "    ### Please use the provided softmax function (imported earlier in this file)\n",
    "    ### This numerically stable implementation helps you avoid issues pertaining\n",
    "    ### to integer overflow. \n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "\n",
    "def getNegativeSamples(outsideWordIdx, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the outsideWordIdx \"\"\"\n",
    "\n",
    "    negSampleWordIndices = [None] * K\n",
    "    for k in range(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == outsideWordIdx:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        negSampleWordIndices[k] = newidx\n",
    "    return negSampleWordIndices\n",
    "\n",
    "\n",
    "def negSamplingLossAndGradient(\n",
    "    centerWordVec,\n",
    "    outsideWordIdx,\n",
    "    outsideVectors,\n",
    "    dataset,\n",
    "    K=10\n",
    "):\n",
    "    \"\"\" Negative sampling loss function for word2vec models\n",
    "\n",
    "    Implement the negative sampling loss and gradients for a centerWordVec\n",
    "    and a outsideWordIdx word vector as a building block for word2vec\n",
    "    models. K is the number of negative samples to take.\n",
    "\n",
    "    Note: The same word may be negatively sampled multiple times. For\n",
    "    example if an outside word is sampled twice, you shall have to\n",
    "    double count the gradient with respect to this word. Thrice if\n",
    "    it was sampled three times, and so forth.\n",
    "\n",
    "    Arguments/Return Specifications: same as naiveSoftmaxLossAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Negative sampling of words is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "\n",
    "    ### YOUR CODE HERE (~10 Lines)\n",
    "\n",
    "    ### Please use your implementation of sigmoid in here.\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return loss, gradCenterVec, gradOutsideVecs\n",
    "\n",
    "\n",
    "def skipgram(currentCenterWord, windowSize, outsideWords, word2Ind,\n",
    "             centerWordVectors, outsideVectors, dataset,\n",
    "             word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currentCenterWord -- a string of the current center word\n",
    "    windowSize -- integer, context window size\n",
    "    outsideWords -- list of no more than 2*windowSize strings, the outside words\n",
    "    word2Ind -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    centerWordVectors -- center word vectors (as rows) is in shape \n",
    "                        (num words in vocab, word vector length) \n",
    "                        for all words in vocab (V in pdf handout)\n",
    "    outsideVectors -- outside vectors is in shape \n",
    "                        (num words in vocab, word vector length) \n",
    "                        for all words in vocab (U in the pdf handout)\n",
    "    word2vecLossAndGradient -- the loss and gradient function for\n",
    "                               a prediction vector given the outsideWordIdx\n",
    "                               word vectors, could be one of the two\n",
    "                               loss functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    loss -- the loss function value for the skip-gram model\n",
    "            (J in the pdf handout)\n",
    "    gradCenterVec -- the gradient with respect to the center word vector\n",
    "                     in shape (word vector length, )\n",
    "                     (dJ / dv_c in the pdf handout)\n",
    "    gradOutsideVecs -- the gradient with respect to all the outside word vectors\n",
    "                    in shape (num words in vocab, word vector length) \n",
    "                    (dJ / dU)\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0.0\n",
    "    gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "    gradOutsideVectors = np.zeros(outsideVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE (~8 Lines)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return loss, gradCenterVecs, gradOutsideVectors\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, word2Ind, wordVectors, dataset, \n",
    "                         windowSize,\n",
    "                         word2vecLossAndGradient=naiveSoftmaxLossAndGradient):\n",
    "    batchsize = 50\n",
    "    loss = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    centerWordVectors = wordVectors[:int(N/2),:]\n",
    "    outsideVectors = wordVectors[int(N/2):,:]\n",
    "    for i in range(batchsize):\n",
    "        windowSize1 = random.randint(1, windowSize)\n",
    "        centerWord, context = dataset.getRandomContext(windowSize1)\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerWord, windowSize1, context, word2Ind, centerWordVectors,\n",
    "            outsideVectors, dataset, word2vecLossAndGradient\n",
    "        )\n",
    "        loss += c / batchsize\n",
    "        grad[:int(N/2), :] += gin / batchsize\n",
    "        grad[int(N/2):, :] += gout / batchsize\n",
    "\n",
    "    return loss, grad\n",
    "\n",
    "\n",
    "def test_word2vec():\n",
    "    \"\"\" Test the two word2vec implementations, before running on Stanford Sentiment Treebank \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in range(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with naiveSoftmaxLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, naiveSoftmaxLossAndGradient),\n",
    "        dummy_vectors, \"naiveSoftmaxLossAndGradient Gradient\")\n",
    "    grad_tests_softmax(skipgram, dummy_tokens, dummy_vectors, dataset)\n",
    "\n",
    "    print(\"==== Gradient check for skip-gram with negSamplingLossAndGradient ====\")\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingLossAndGradient),\n",
    "        dummy_vectors, \"negSamplingLossAndGradient Gradient\")\n",
    "\n",
    "    grad_tests_negsamp(skipgram, dummy_tokens, dummy_vectors, dataset, negSamplingLossAndGradient)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (4 points)\n",
    "\n",
    "Complete the implementationm for your SGD optimizer in the *sgd* method of *sgd.py*.  Test your implementation by running python *sgd.py*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 5000\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        params_file = \"saved_params_%d.npy\" % st\n",
    "        state_file = \"saved_state_%d.pickle\" % st\n",
    "        params = np.load(params_file)\n",
    "        with open(state_file, \"rb\") as f:\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "\n",
    "\n",
    "def save_params(iter, params):\n",
    "    params_file = \"saved_params_%d.npy\" % iter\n",
    "    np.save(params_file, params)\n",
    "    with open(\"saved_state_%d.pickle\" % iter, \"wb\") as f:\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
    "        PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a loss and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD\n",
    "    iterations -- total iterations to run SGD for\n",
    "    postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "\n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "\n",
    "    exploss = None\n",
    "\n",
    "    for iter in range(start_iter + 1, iterations + 1):\n",
    "        # You might want to print the progress every few iterations.\n",
    "\n",
    "        loss = None\n",
    "        ### YOUR CODE HERE (~2 lines)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        x = postprocessing(x)\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not exploss:\n",
    "                exploss = loss\n",
    "            else:\n",
    "                exploss = .95 * exploss + .05 * loss\n",
    "            print(\"iter %d: %f\" % (iter, exploss))\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def sanity_check():\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print(\"Running sanity checks...\")\n",
    "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print(\"test 1 result:\", t1)\n",
    "    assert abs(t1) <= 1e-6\n",
    "\n",
    "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print(\"test 2 result:\", t2)\n",
    "    assert abs(t2) <= 1e-6\n",
    "\n",
    "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print(\"test 3 result:\", t3)\n",
    "    assert abs(t3) <= 1e-6\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(\"ALL TESTS PASSED\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) (4 points)\n",
    "\n",
    "Show time! Now we are going to load some real data and train word vectors with everything you've just implemented!  We are going to use the Stanford Sentiment Treebank (SST) dataset to train word vectors, and later apply them to a simple sentiment analysis task.  You will need to fetch the datasets first.  To do this, run:\n",
    "\n",
    "```sh get_datasets.sh```\n",
    "\n",
    "There is no additional code to write for this; just run:\n",
    "\n",
    "```python run.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset to work with in following code.\n",
    "! sh get_datasets.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>\n",
    "\n",
    "The training process may take a long time depending on the efficiency of your implementation and the compute power of your machine (**an efficient implementation takes one to two hours**), plan accordingly!\n",
    "\n",
    "</p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 40,000 iterations, the script will finish and a visualization for your word vectors will appear.  It will also be saved as *word_vectors.png* in your project directory.  **Include the plot in your homework write up**.  Briefly explain in the most three sentences what you see in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.treebank import StanfordSentiment\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Below removed because these can be run from within the Notebook without importing.\n",
    "#from word2vec import *\n",
    "#from sgd import *\n",
    "\n",
    "# Check Python Version\n",
    "import sys\n",
    "assert sys.version_info[0] == 3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "startTime=time.time()\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "        negSamplingLossAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# Note that normalization is not called here. This is not a bug,\n",
    "# normalizing during training loses the notion of length.\n",
    "\n",
    "print(\"sanity check: cost at convergence should be around or below 10\")\n",
    "print(\"training took %d seconds\" % (time.time() - startTime))\n",
    "\n",
    "# concatenate the input and output word vectors\n",
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "    axis=0)\n",
    "\n",
    "visualizeWords = [\n",
    "    \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"dumb\",\n",
    "    \"annoying\", \"female\", \"male\", \"queen\", \"king\", \"man\", \"woman\", \"rain\", \"snow\",\n",
    "    \"hail\", \"coffee\", \"tea\"]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])\n",
    "\n",
    "for i in range(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('word_vectors.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
